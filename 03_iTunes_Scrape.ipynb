{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycurl, json, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from common_func import check_url\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import urllib2\n",
    "import unidecode\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49930, 36)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkls = !ls pkl/itunes*COMPLETE.pkl\n",
    "podcastDf = pd.read_pickle(pkls[-1])\n",
    "podcastDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33227, 36)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out podcasts with no itunes collection id\n",
    "podcastDf = podcastDf[np.isfinite(podcastDf['collectionId'])]\n",
    "podcastDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6273, 36)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out podcasts without recent episodes\n",
    "days_thresh = 45\n",
    "thresh_date = dt.datetime.today() - dt.timedelta(days=days_thresh)\n",
    "\n",
    "podcastDf['releaseDate'] = pd.to_datetime(podcastDf['releaseDate'])\n",
    "podcastDf = podcastDf[podcastDf.releaseDate > thresh_date]\n",
    "podcastDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert collectionId to int\n",
    "podcastDf['collectionId'] = [int(x) for x in podcastDf['collectionId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sample for testing\n",
    "testDf = podcastDf.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseUrl = 'https://itunes.apple.com/us/podcast/id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will store pycurl output\n",
    "class Test:\n",
    "   def __init__(self):\n",
    "       self.contents = ''\n",
    "\n",
    "   def body_callback(self, buf):\n",
    "       self.contents = self.contents + buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_curl(url):\n",
    "    t = Test()\n",
    "    c = pycurl.Curl()\n",
    "    c.setopt(pycurl.URL, url)\n",
    "    c.setopt(pycurl.HTTPHEADER, ['X-Apple-Store-Front: 143441-1,12', 'X-Apple-Tz: 3600'])\n",
    "    c.setopt(pycurl.USERAGENT, 'iTunes/9.2.1 (Macintosh; Intel Mac OS X 10.5.8) AppleWebKit/533.16')\n",
    "    c.setopt(pycurl.SSL_VERIFYHOST, 0)\n",
    "    c.setopt(pycurl.SSL_VERIFYPEER, 0)\n",
    "    c.setopt(pycurl.WRITEFUNCTION, t.body_callback)\n",
    "    c.perform()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_description(d):\n",
    "    d = unidecode.unidecode(d)\n",
    "    d = d.replace('\\n', ' ')\n",
    "    if re.findall(r'(.*) brought to you by.*', d):\n",
    "       d = re.sub(r'brought to you by.*', '', d)\n",
    "    if re.search(r'(.*) sponsored by.*', d):\n",
    "       d = re.sub(r'sponsored by.*', '', d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1147/6273 [1:03:01<15:24:29, 10.82s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-b7ad5246d9a9>\u001b[0m in \u001b[0;36mbody_callback\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m      4\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mbody_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "error",
     "evalue": "(23, 'Failed writing body (0 != 16374)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-461-d07c39634d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# get podcast summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_curl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapeUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-313-499ef3ff21b6>\u001b[0m in \u001b[0;36mrun_curl\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpycurl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_VERIFYPEER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpycurl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRITEFUNCTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: (23, 'Failed writing body (0 != 16374)')"
     ]
    }
   ],
   "source": [
    "colNames = ['collectionId', 'podcastSummary', 'episodeNames', 'episodeDescriptions', 'alsoSubscribed']\n",
    "scrapeResults = pd.DataFrame(columns=colNames)\n",
    "\n",
    "for ind, row in tqdm.tqdm(podcastDf.iterrows(), total=podcastDf.shape[0]):\n",
    "    collectionId = row['collectionId']\n",
    "    scrapeUrl = baseUrl + str(collectionId)\n",
    "    \n",
    "    # get podcast summary\n",
    "    t = run_curl(scrapeUrl)\n",
    "    soup = BeautifulSoup(t.contents)\n",
    "    p = soup.p\n",
    "    if p:\n",
    "        podcastSummary = soup.p.string\n",
    "    else: # redirect\n",
    "        newUrl = soup.findAll(text=re.compile(r'https'))\n",
    "        newUrl = newUrl[0]\n",
    "        newUrl = re.sub(r'&amp;', r'&', newUrl)\n",
    "        try:\n",
    "            t = run_curl(newUrl)\n",
    "            soup = BeautifulSoup(t.contents)\n",
    "            p = soup.p\n",
    "            if p:\n",
    "                podcastSummary = soup.p.string\n",
    "            else:\n",
    "                podcastSummary = np.nan\n",
    "                episodeNames = np.nan\n",
    "                episodeDescriptions = np.nan\n",
    "                alsoSubscribed = np.nan\n",
    "                thisResult = pd.DataFrame({'collectionId' : int(collectionId),\n",
    "                                          'podcastSummary' : [podcastSummary],\n",
    "                                          'episodeNames' : [episodeNames],\n",
    "                                          'episodeDescriptions' : [episodeDescriptions],\n",
    "                                          'alsoSubscribed' : [alsoSubscribed]})\n",
    "                scrapeResults = pd.concat([scrapeResults, thisResult], axis=0)\n",
    "                continue\n",
    "        except:\n",
    "            podcastSummary = np.nan\n",
    "            episodeNames = np.nan\n",
    "            episodeDescriptions = np.nan\n",
    "            alsoSubscribed = np.nan\n",
    "            thisResult = pd.DataFrame({'collectionId' : int(collectionId),\n",
    "                                      'podcastSummary' : [podcastSummary],\n",
    "                                      'episodeNames' : [episodeNames],\n",
    "                                      'episodeDescriptions' : [episodeDescriptions],\n",
    "                                      'alsoSubscribed' : [alsoSubscribed]})\n",
    "            scrapeResults = pd.concat([scrapeResults, thisResult], axis=0)\n",
    "            continue\n",
    "    \n",
    "    # get episode names\n",
    "    episodeData = soup.findAll('button', kind='episode')\n",
    "    try:\n",
    "        episodeNames = [unidecode.unidecode(e['item-name']) for e in episodeData]\n",
    "    except: # no name\n",
    "        episodeNames = np.nan\n",
    "    \n",
    "    # get episode descriptions\n",
    "    try:\n",
    "        episodeDescriptions = [clean_description(e['description']) for e in episodeData]\n",
    "    except: # no description\n",
    "        episodeDescriptions = np.nan\n",
    "    \n",
    "    # get also subscribed podcasts\n",
    "    alsoSubscribed = re.findall(r'adam-id=\"(\\d+)\" aria-label=', t.contents)\n",
    "    try:\n",
    "        alsoSubscribed = [int(x) for x in alsoSubscribed]\n",
    "    except:\n",
    "        alsoSubscribed = np.nan\n",
    "        \n",
    "    # append results\n",
    "    thisResult = pd.DataFrame({'collectionId' : int(collectionId),\n",
    "                                      'podcastSummary' : [podcastSummary],\n",
    "                                      'episodeNames' : [episodeNames],\n",
    "                                      'episodeDescriptions' : [episodeDescriptions],\n",
    "                                      'alsoSubscribed' : [alsoSubscribed]})\n",
    "    scrapeResults = pd.concat([scrapeResults, thisResult], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle the current results\n",
    "scrapeResults['episodeDescriptions'] = [unicode(x) for x in scrapeResults['episodeDescriptions']]\n",
    "scrapeResults['episodeNames'] = [unicode(x) for x in scrapeResults['episodeNames']]\n",
    "scrapeResults['podcastSummary'] = [unicode(x) for x in scrapeResults['podcastSummary']]\n",
    "scrapeResults.to_pickle('pkl/scraped_podcasts_pt1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove already-retrieved values from podcastDf\n",
    "doneIds = scrapeResults['collectionId']\n",
    "\n",
    "subDf = podcastDf[~podcastDf.collectionId.isin(doneIds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5126, 36)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ind, row in tqdm.tqdm(subDf.iterrows(), total=subDf.shape[0]):\n",
    "    collectionId = row['collectionId']\n",
    "    scrapeUrl = baseUrl + str(collectionId)\n",
    "    \n",
    "    # get podcast summary\n",
    "    t = run_curl(scrapeUrl)\n",
    "    soup = BeautifulSoup(t.contents)\n",
    "    p = soup.p\n",
    "    if p:\n",
    "        podcastSummary = soup.p.string\n",
    "    else: # redirect\n",
    "        newUrl = soup.findAll(text=re.compile(r'https'))\n",
    "        newUrl = newUrl[0]\n",
    "        newUrl = re.sub(r'&amp;', r'&', newUrl)\n",
    "        try:\n",
    "            t = run_curl(newUrl)\n",
    "            soup = BeautifulSoup(t.contents)\n",
    "            p = soup.p\n",
    "            if p:\n",
    "                podcastSummary = soup.p.string\n",
    "            else:\n",
    "                podcastSummary = np.nan\n",
    "                episodeNames = np.nan\n",
    "                episodeDescriptions = np.nan\n",
    "                alsoSubscribed = np.nan\n",
    "                thisResult = pd.DataFrame({'collectionId' : int(collectionId),\n",
    "                                          'podcastSummary' : [podcastSummary],\n",
    "                                          'episodeNames' : [episodeNames],\n",
    "                                          'episodeDescriptions' : [episodeDescriptions],\n",
    "                                          'alsoSubscribed' : [alsoSubscribed]})\n",
    "                scrapeResults = pd.concat([scrapeResults, thisResult], axis=0)\n",
    "                continue\n",
    "        except:\n",
    "            podcastSummary = np.nan\n",
    "            episodeNames = np.nan\n",
    "            episodeDescriptions = np.nan\n",
    "            alsoSubscribed = np.nan\n",
    "            thisResult = pd.DataFrame({'collectionId' : int(collectionId),\n",
    "                                      'podcastSummary' : [podcastSummary],\n",
    "                                      'episodeNames' : [episodeNames],\n",
    "                                      'episodeDescriptions' : [episodeDescriptions],\n",
    "                                      'alsoSubscribed' : [alsoSubscribed]})\n",
    "            scrapeResults = pd.concat([scrapeResults, thisResult], axis=0)\n",
    "            continue\n",
    "    \n",
    "    # get episode names\n",
    "    episodeData = soup.findAll('button', kind='episode')\n",
    "    try:\n",
    "        episodeNames = [unidecode.unidecode(e['item-name']) for e in episodeData]\n",
    "    except: # no name\n",
    "        episodeNames = np.nan\n",
    "    \n",
    "    # get episode descriptions\n",
    "    try:\n",
    "        episodeDescriptions = [clean_description(e['description']) for e in episodeData]\n",
    "    except: # no description\n",
    "        episodeDescriptions = np.nan\n",
    "    \n",
    "    # get also subscribed podcasts\n",
    "    alsoSubscribed = re.findall(r'adam-id=\"(\\d+)\" aria-label=', t.contents)\n",
    "    try:\n",
    "        alsoSubscribed = [int(x) for x in alsoSubscribed]\n",
    "    except:\n",
    "        alsoSubscribed = np.nan\n",
    "        \n",
    "    # append results\n",
    "    thisResult = pd.DataFrame({'collectionId' : int(collectionId),\n",
    "                                      'podcastSummary' : [podcastSummary],\n",
    "                                      'episodeNames' : [episodeNames],\n",
    "                                      'episodeDescriptions' : [episodeDescriptions],\n",
    "                                      'alsoSubscribed' : [alsoSubscribed]})\n",
    "    scrapeResults = pd.concat([scrapeResults, thisResult], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
